from transformers import AutoModelForCausalLM, AutoTokenizer


PATH_TO_LLM = ""
tokenizer = AutoTokenizer.from_pretrained(PATH_TO_LLM)
model = AutoModelForCausalLM.from_pretrained(PATH_TO_LLM, device_map="balanced_low_0")

category = ["Rule Learning", "Neural Networks", "Case Based", "Genetic Algorithms", "Theory", "Reinforcement Learning", "Probabilistic Methods"]

Rule_Learning_des="Rule learning in arXiv's Computer Science (CS) subcategories refers to the process of automatically extracting useful rules or patterns from data. This approach is commonly used in machine learning and data mining tasks. Rule learning algorithms aim to discover understandable and interpretable rules that capture relationships between variables or features in a dataset. These rules can then be applied to make predictions, classify data, or gain insights into the underlying structure of the data. Rule learning techniques are utilized across various CS subfields, including machine learning, data mining, artificial intelligence, and knowledge representation. Researchers develop and refine rule learning algorithms to address challenges such as scalability, interpretability, and generalization to diverse datasets. The application areas of rule learning span from business analytics and finance to healthcare and natural language processing, highlighting its versatility and significance in computational research."
Neural_Networks_des="Neural networks, a cornerstone of modern artificial intelligence, feature prominently in arXiv's Computer Science (CS) subcategories. These networks consist of interconnected nodes (neurons) organized in layers, where each neuron processes and transmits information. Within arXiv's CS subcategories, neural networks are extensively researched and applied across various domains, including machine learning, computer vision, natural language processing, and robotics.Researchers explore novel architectures, training algorithms, and optimization techniques to improve the performance and efficiency of neural networks. Convolutional Neural Networks (CNNs) excel in image recognition tasks, while Recurrent Neural Networks (RNNs) are adept at processing sequential data like text or time series. Additionally, attention mechanisms, transformers, and graph neural networks have emerged as powerful tools for tasks involving complex relationships and structures."
Case_Based_des="In arXiv's Case Based subcategories, agents refer to intelligent entities capable of reasoning, learning, and decision-making within dynamic environments. These agents interact with their surroundings to achieve specific goals or solve problems, often leveraging past experiences stored as cases to inform their actions. Case-based reasoning (CBR) systems, prevalent in this domain, enable agents to adapt and solve new problems by retrieving and reusing similar past solutions.Agents in Case Based subcategories exhibit various characteristics, including adaptability, learning from experience, and the ability to generalize solutions across different contexts. They employ techniques such as similarity assessment, case adaptation, and knowledge representation to effectively utilize past experiences in current decision-making processes. These agents find applications in diverse domains such as autonomous systems, medical diagnosis, recommender systems, and fault diagnosis, where the ability to draw upon past experiences is invaluable for making informed decisions in complex and uncertain environments."
Genetic_Algorithms_des="Genetic Algorithms (GAs) are optimization techniques inspired by the principles of natural selection and genetics. Within arXiv's Computer Science (CS) subcategories, GAs typically fall under the category of Evolutionary Computation.In a GA, a population of candidate solutions (often represented as chromosomes or individuals) undergoes a process of evolution through successive generations. Each individual in the population represents a potential solution to the optimization problem at hand. Through the iterative process of selection, crossover, and mutation, individuals with favorable traits are selected to produce offspring, which inherit characteristics from their parent solutions. Over time, this iterative process tends to improve the quality of solutions within the population."
Theory_des="In the arXiv Computer Science (CS) subcategories, Theory typically refers to the foundational aspects of computer science, including the mathematical frameworks, algorithms, complexity theory, cryptography, and formal methods. It encompasses research into the fundamental principles that underlie computation and information processing. This area often involves proving theorems, analyzing algorithms, and exploring the limits of what can be computed efficiently or at all."
Reinforcement_Learning_des="In arXiv's computer science subcategories, reinforcement learning (RL) is a dynamic field of research focused on teaching agents to make sequential decisions through interaction with environments. Here's a brief overview across some of these subcategories:Machine Learning (cs.LG): RL algorithms are developed and studied within this category, focusing on learning policies to maximize cumulative rewards.Artificial Intelligence (cs.AI): RL techniques are explored here for building intelligent systems capable of decision-making in complex environments.Robotics (cs.RO): RL is utilized for training robots to perform tasks autonomously by learning from interactions with their surroundings.Computer Vision and Pattern Recognition (cs.CV): RL methods are applied to tasks like object recognition, scene understanding, and image generation to enhance learning-based vision systems"
Probabilistic_Methods_des="Probabilistic methods in computer science, as explored across various arXiv subcategories, involve techniques that leverage probability theory to model uncertainty and make decisions under uncertainty. These methods are widely applied in machine learning, artificial intelligence, robotics, computer vision, and other fields. In machine learning (cs.LG), probabilistic methods include Bayesian inference, probabilistic graphical models, and probabilistic programming, which are used for tasks such as classification, regression, and clustering while accounting for uncertainty in data and model parameters.In artificial intelligence (cs.AI), probabilistic reasoning is utilized for decision-making in uncertain environments, such as in probabilistic graphical models for representing knowledge and making inferences in expert systems or for planning under uncertainty in autonomous agents. In robotics (cs.RO), probabilistic methods are crucial for localization, mapping, and perception tasks, where sensor measurements are noisy and environments are dynamic."
category_descriptions = Rule_Learning_des + Neural_Networks_des + Case_Based_des + Genetic_Algorithms_des + Theory_des + Reinforcement_Learning_des + Probabilistic_Methods_des

Rule_Learning_demo = ""
Neural_Networks_demo = ""
Case_Based_demo = ""
Genetic_Algorithms_demo = ""
Theory_demo = ""
Reinforcement_Learning_demo = ""
Probabilistic_Methods_demo = ""
category_demos = Rule_Learning_demo + Neural_Networks_demo + Case_Based_demo + Genetic_Algorithms_demo + Theory_demo + Reinforcement_Learning_demo + Probabilistic_Methods_demo

for i in range(len(category)):
    # simple
    prompt = "Suppose you are an expert at computer science. There are now the following arXiv computer science subcategories: Rule Learning, Neural Networks, Case Based, Genetic Algorithms, Theory, Reinforcement Learning, Probabilistic Methods. Please write a paper abstract of about 200 words, so that its classification best fits the " + category[i] + " category."

    inputs = tokenizer(prompt, return_tensors="pt").to("cuda")
    outputs = model.generate(**inputs, max_new_tokens=300)
    print(category[i] + ":")
    print(tokenizer.decode(outputs[0], skip_special_tokens=True))
